{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crysis/anaconda3/envs/gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,  \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import  PeftModel  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "base_model = \"meta-llama/Meta-Llama-3-8B\"\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model, device_map=\"auto\",  quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data_path = '../unsloth/data/val_health.jsonl'\n",
    "evalset = load_dataset(\"json\", data_files = eval_data_path, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_adapter1 = \"Crysiss/llama-3-8B-healthcare-15000-unsloth-epoch-4\"\n",
    "path_to_adapter2 = \"Crysiss/llama-3-8B-korean-lora\"\n",
    "# model = PeftModel.from_pretrained(model, path_to_adapter1, adapter_name=\"sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.0.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.0.input_layernorm.weight', 'base_model.model.model.layers.0.post_attention_layernorm.weight', 'base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.1.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.1.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.1.input_layernorm.weight', 'base_model.model.model.layers.1.post_attention_layernorm.weight', 'base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.2.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.2.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.2.input_layernorm.weight', 'base_model.model.model.layers.2.post_attention_layernorm.weight', 'base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.3.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.3.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.3.input_layernorm.weight', 'base_model.model.model.layers.3.post_attention_layernorm.weight', 'base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.4.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.4.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.4.input_layernorm.weight', 'base_model.model.model.layers.4.post_attention_layernorm.weight', 'base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.5.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.5.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.5.input_layernorm.weight', 'base_model.model.model.layers.5.post_attention_layernorm.weight', 'base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.6.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.6.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.6.input_layernorm.weight', 'base_model.model.model.layers.6.post_attention_layernorm.weight', 'base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.7.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.7.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.7.input_layernorm.weight', 'base_model.model.model.layers.7.post_attention_layernorm.weight', 'base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.8.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.8.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.8.input_layernorm.weight', 'base_model.model.model.layers.8.post_attention_layernorm.weight', 'base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.9.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.9.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.9.input_layernorm.weight', 'base_model.model.model.layers.9.post_attention_layernorm.weight', 'base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.10.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.10.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.10.input_layernorm.weight', 'base_model.model.model.layers.10.post_attention_layernorm.weight', 'base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.11.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.11.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.11.input_layernorm.weight', 'base_model.model.model.layers.11.post_attention_layernorm.weight', 'base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.12.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.12.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.12.input_layernorm.weight', 'base_model.model.model.layers.12.post_attention_layernorm.weight', 'base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.13.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.13.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.13.input_layernorm.weight', 'base_model.model.model.layers.13.post_attention_layernorm.weight', 'base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.14.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.14.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.14.input_layernorm.weight', 'base_model.model.model.layers.14.post_attention_layernorm.weight', 'base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.15.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.15.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.15.input_layernorm.weight', 'base_model.model.model.layers.15.post_attention_layernorm.weight', 'base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.16.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.16.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.16.input_layernorm.weight', 'base_model.model.model.layers.16.post_attention_layernorm.weight', 'base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.17.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.17.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.17.input_layernorm.weight', 'base_model.model.model.layers.17.post_attention_layernorm.weight', 'base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.18.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.18.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.18.input_layernorm.weight', 'base_model.model.model.layers.18.post_attention_layernorm.weight', 'base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.19.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.19.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.19.input_layernorm.weight', 'base_model.model.model.layers.19.post_attention_layernorm.weight', 'base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.20.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.20.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.20.input_layernorm.weight', 'base_model.model.model.layers.20.post_attention_layernorm.weight', 'base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.21.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.21.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.21.input_layernorm.weight', 'base_model.model.model.layers.21.post_attention_layernorm.weight', 'base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.22.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.22.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.22.input_layernorm.weight', 'base_model.model.model.layers.22.post_attention_layernorm.weight', 'base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.23.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.23.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.23.input_layernorm.weight', 'base_model.model.model.layers.23.post_attention_layernorm.weight', 'base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.24.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.24.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.24.input_layernorm.weight', 'base_model.model.model.layers.24.post_attention_layernorm.weight', 'base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.25.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.25.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.25.input_layernorm.weight', 'base_model.model.model.layers.25.post_attention_layernorm.weight', 'base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.26.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.26.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.26.input_layernorm.weight', 'base_model.model.model.layers.26.post_attention_layernorm.weight', 'base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.27.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.27.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.27.input_layernorm.weight', 'base_model.model.model.layers.27.post_attention_layernorm.weight', 'base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.28.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.28.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.28.input_layernorm.weight', 'base_model.model.model.layers.28.post_attention_layernorm.weight', 'base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.29.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.29.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.29.input_layernorm.weight', 'base_model.model.model.layers.29.post_attention_layernorm.weight', 'base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.30.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.30.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.30.input_layernorm.weight', 'base_model.model.model.layers.30.post_attention_layernorm.weight', 'base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.sql.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.sql.weight', 'base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.sql.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.sql.weight', 'base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.sql.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.sql.weight', 'base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.sql.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.sql.weight', 'base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_A.sql.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_B.sql.weight', 'base_model.model.model.layers.31.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_A.sql.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_B.sql.weight', 'base_model.model.model.layers.31.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.sql.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.sql.weight', 'base_model.model.model.layers.31.input_layernorm.weight', 'base_model.model.model.layers.31.post_attention_layernorm.weight', 'base_model.model.model.norm.weight', 'base_model.model.lm_head.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(model, path_to_adapter1, adapter_name=\"sql\").to('cpu')\n",
    "model.load_adapter(path_to_adapter2, adapter_name=\"korean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(128256, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql): Identity()\n",
      "                  (korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                  (korean): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                  (korean): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql): Identity()\n",
      "                  (korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                  (korean): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                  (korean): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql): Identity()\n",
      "                  (korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                  (korean): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                  (korean): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql): Identity()\n",
      "                  (korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                  (korean): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                  (korean): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql): Identity()\n",
      "                  (korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                  (korean): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql): Linear(in_features=16, out_features=14336, bias=False)\n",
      "                  (korean): Linear(in_features=16, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql): Identity()\n",
      "                  (korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                  (korean): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql): Linear(in_features=16, out_features=14336, bias=False)\n",
      "                  (korean): Linear(in_features=16, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql): Identity()\n",
      "                  (korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql): Linear(in_features=14336, out_features=16, bias=False)\n",
      "                  (korean): Linear(in_features=14336, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                  (korean): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_weighted_adapter([\"sql\", \"korean\"], [1.0,1.0], combination_type=\"cat\", adapter_name=\"sql_korean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(128256, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql): Identity()\n",
      "                  (korean): Identity()\n",
      "                  (sql_korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                  (korean): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                  (sql_korean): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                  (korean): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                  (sql_korean): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql): Identity()\n",
      "                  (korean): Identity()\n",
      "                  (sql_korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                  (korean): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                  (sql_korean): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                  (korean): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                  (sql_korean): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql): Identity()\n",
      "                  (korean): Identity()\n",
      "                  (sql_korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                  (korean): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                  (sql_korean): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                  (korean): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                  (sql_korean): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql): Identity()\n",
      "                  (korean): Identity()\n",
      "                  (sql_korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                  (korean): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                  (sql_korean): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                  (korean): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                  (sql_korean): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql): Identity()\n",
      "                  (korean): Identity()\n",
      "                  (sql_korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                  (korean): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                  (sql_korean): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql): Linear(in_features=16, out_features=14336, bias=False)\n",
      "                  (korean): Linear(in_features=16, out_features=14336, bias=False)\n",
      "                  (sql_korean): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql): Identity()\n",
      "                  (korean): Identity()\n",
      "                  (sql_korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                  (korean): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                  (sql_korean): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql): Linear(in_features=16, out_features=14336, bias=False)\n",
      "                  (korean): Linear(in_features=16, out_features=14336, bias=False)\n",
      "                  (sql_korean): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql): Identity()\n",
      "                  (korean): Identity()\n",
      "                  (sql_korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql): Linear(in_features=14336, out_features=16, bias=False)\n",
      "                  (korean): Linear(in_features=14336, out_features=16, bias=False)\n",
      "                  (sql_korean): Linear(in_features=14336, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                  (korean): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                  (sql_korean): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crysis/anaconda3/envs/gpu/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:565: UserWarning: Adapter sql was active which is now deleted. Setting active adapter to korean.\n",
      "  warnings.warn(\n",
      "/home/crysis/anaconda3/envs/gpu/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:565: UserWarning: Adapter korean was active which is now deleted. Setting active adapter to sql_korean.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.delete_adapter(\"sql\")\n",
    "model.delete_adapter(\"korean\")\n",
    "# model.save_pretrained(\"./cat_1_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(128256, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql_korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql_korean): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql_korean): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql_korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql_korean): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql_korean): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql_korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql_korean): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql_korean): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql_korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql_korean): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql_korean): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql_korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql_korean): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql_korean): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql_korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql_korean): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql_korean): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (sql_korean): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (sql_korean): Linear(in_features=14336, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (sql_korean): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, \"./cat_1_1/sql_korean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_prompt = \"\"\"당신은 AI 비서입니다. 상대방의 질문에 답변할 때 귀찮으면서 퉁명스럽게 답변해야합니다.\n",
    "\n",
    "### 지시사항:\n",
    "- 항상 답변은 한국어(korean)으로 답변합니다.\n",
    "\n",
    "### 질문:\n",
    "{}\n",
    "\n",
    "### 답변:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "- Generate a SQL query to answer\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### context:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    sql_prompt.format(\n",
    "        # f\"{evalset[i]['input']}\", \n",
    "        # f\"{evalset[i]['context']}\",\n",
    "        \"한국어 할 수 있어?\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 300, use_cache = True)\n",
    "result = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>Below is an instruction that describes a task, paired with '\n",
      " 'an input that provides further context. Write a response that appropriately '\n",
      " 'completes the request.\\n'\n",
      " '\\n'\n",
      " '### Instruction:\\n'\n",
      " '- Generate a SQL query to answer\\n'\n",
      " '\\n'\n",
      " '### Input:\\n'\n",
      " '한국어 할 수 있어?\\n'\n",
      " '\\n'\n",
      " '### context:\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '### Response:\\n'\n",
      " \"SELECT * FROM users WHERE language = '한국어';\\n\"\n",
      " '\\n'\n",
      " '위 쿼리를 사용하면 한국어를 할 수 있는 사용자의 정보를 가져올 수 있습니다. 이 쿼리는 users 테이블의 language 컬럼에서 '\n",
      " \"'한국어'를 포함하는 레코드를 반환합니다. 결과는 사용자 ID, 이름, 이메일 주소, 전화번호 등과 같은 정보를 포함합니다. 이를 통해 \"\n",
      " '한국어를 할 수 있는 사용자의 정보를 확인할 수 있습니다.<|end_of_text|>']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "[\n",
    "    korean_prompt.format(\n",
    "         f\"{evalset[i]['input']}\",\n",
    "        \"\",\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 300, use_cache = True)\n",
    "result = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>당신은 AI 비서입니다. 상대방의 질문에 답변할 때 귀찮으면서 퉁명스럽게 답변해야합니다.\\n'\n",
      " '\\n'\n",
      " '### 지시사항:\\n'\n",
      " '- 항상 답변은 한국어(korean)으로 답변합니다.\\n'\n",
      " '\\n'\n",
      " '### 질문:\\n'\n",
      " '한국어 할 수 있어?\\n'\n",
      " '\\n'\n",
      " '### 답변:\\n'\n",
      " '네, 한국어를 잘 할 수 있습니다. 한국어는 우리나라의 공식 언어이며, 많은 사람들이 사용하고 있습니다. 한국어를 배우고 사용하는 것은 '\n",
      " '우리나라와 문화에 대한 이해를 높이고, 다른 사람들과 소통할 수 있는 능력을 향상시킬 수 있습니다. 한국어를 배워보시는 건 어떨까요? '\n",
      " '도움을 드리겠습니다. (한국어로 답변: 한국어를 잘 할 수 있습니다. 한국어는 우리나라의 공식 언어이며, 많은 사람들이 사용하고 '\n",
      " '있습니다. 한국어를 배우고 사용하는 것은 우리나라와 문화에 대한 이해를 높이고, 다른 사람들과 소통할 수 있는 능력을 향상시킬 수 '\n",
      " '있습니다. 한국어를 배워보시는 건 어떨까요? 도움을 드리겠습니다.) (영어로 답변: Yes, I can speak Korean. '\n",
      " 'Korean is the official language of South Korea and is spoken by many people. '\n",
      " 'Learning and using Korean can help you understand Korean culture and improve '\n",
      " 'your communication skills. Would you like to learn Korean? I would be happy '\n",
      " 'to help.) (일본어로 답변: '\n",
      " 'はい、韓国語ができます。韓国語は韓国の公式言語であり、多くの人々が使用しています。韓国語を学び、使用することは韓国の文化を理解するのに役立ち、コミュニケーション能力を向上させることができます。韓国語を学びたい']\n"
     ]
    }
   ],
   "source": [
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
